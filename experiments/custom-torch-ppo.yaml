procgen-ppo:
    env: frame_stacked_procgen
#     env: procgen_env_wrapper
    run: CustomTorchPPOAgent
    # === Stop Conditions ===
    stop:
        timesteps_total: 8000000

    # === Settings for Checkpoints ===
    checkpoint_freq: 100
    checkpoint_at_end: True
    keep_checkpoints_num: 5

    config:
        # === Settings for the Procgen Environment ===
        env_config:
            env_name: miner
            num_levels: 0
            start_level: 0
            paint_vel_info: False
            use_generated_assets: False
            center_agent: True
            use_sequential_levels: False
            distribution_mode: easy
#             frame_stack: 2

        # === Environment Settings ===
        # Discount factor of the MDP.
        gamma: 0.999
        # The GAE(lambda) parameter.
        lambda: 0.95
        # The default learning rate.
        lr: 5.0e-4
        # Number of SGD iterations in each outer loop (i.e., number of epochs to
        # execute per train batch).
        num_sgd_iter: 3
        # Total SGD batch size across all devices for SGD. This defines the
        # minibatch size within each epoch.
        sgd_minibatch_size: 2048 # 8 minibatches per epoch
        # Training batch size, if applicable. Should be >= rollout_fragment_length.
        # Samples batches will be concatenated together to a batch of this size,
        # which is then passed to SGD.
#         train_batch_size: 16384 # 256 * 64
        train_batch_size: 18432 # 256 * 72
        # Coefficient of the value function loss.
        vf_loss_coeff: 0.5
        # Coefficient of the entropy regularizer.
        entropy_coeff: 0.01
        # PPO clip parameter.
        clip_param: 0.2
        # Clip param for the value function. Note that this is sensitive to the
        # scale of the rewards. If your expected V is large, increase this.
        vf_clip_param: 0.2
        # If specified, clip the global norm of gradients by this amount.
        grad_clip: 0.5
        # Which observation filter to apply to the observation.
        observation_filter: NoFilter
        # Share layers for value function. If you set this to True, it's important
        # to tune vf_loss_coeff.
        vf_share_layers: True
        # Number of steps after which the episode is forced to terminate. Defaults
        # to `env.spec.max_episode_steps` (if present) for Gym envs.
        horizon: null
        # Calculate rewards but don't reset the environment when the horizon is
        # hit. This allows value estimation and RNN state to span across logical
        # episodes denoted by horizon. This only has an effect if horizon != inf.
        soft_horizon: False
        # Don't set 'done' at the end of the episode. Note that you still need to
        # set this if soft_horizon=True, unless your env is actually running
        # forever without returning done=True.
        no_done_at_end: False

        # Unsquash actions to the upper and lower bounds of env's action space
        normalize_actions: False
        # Whether to clip rewards prior to experience postprocessing. Setting to
        # None means clip for Atari only.
        clip_rewards: null
        # Whether to np.clip() actions to the action space low/high range spec.
        clip_actions: True
        # Whether to use rllib or deepmind preprocessors by default
        preprocessor_pref: deepmind

        # Whether to attempt to continue training if a worker crashes. The number
        # of currently healthy workers is reported as the "num_healthy_workers"
        # metric.
        ignore_worker_failures: False
        # Log system resource metrics to results. This requires `psutil` to be
        # installed for sys stats, and `gputil` for GPU metrics.
        # Note : The AIcrowd Evaluators will always override this to be True
        log_sys_usage: True

        # Use PyTorch (instead of tf). If using `rllib train`, this can also be
        # enabled with the `--torch` flag.
        # NOTE: Some agents may not support `torch` yet and throw an error.
        use_pytorch: True

        # === Settings for Model ===
        model:
            custom_model: impala_torch_custom
            custom_options: {depths: [32, 64, 64]}
            
        evaluation_interval: 25
        evaluation_num_workers: 1
        evaluation_num_episodes: 3
        evaluation_config:
          num_envs_per_worker: 1
          env_config:
            render_mode: rgb_array

        # === Settings for Rollout Worker processes ===
        # Number of rollout worker actors to create for parallel sampling. Setting
        # this to 0 will force rollouts to be done in the trainer actor.
        num_workers: 6

        # Number of environments to evaluate vectorwise per worker. This enables
        # model inference batching, which can improve performance for inference
        # bottlenecked workloads.
        num_envs_per_worker: 12

        # Divide episodes into fragments of this many steps each during rollouts.
        rollout_fragment_length: 256

        # Whether to rollout "complete_episodes" or "truncate_episodes" to
        # `rollout_fragment_length` length unrolls. Episode truncation guarantees
        # evenly sized batches, but increases variance as the reward-to-go will
        # need to be estimated at truncation boundaries.
        batch_mode: truncate_episodes

        # === Advanced Resource Settings ===
        # Number of CPUs to allocate per worker.
        num_cpus_per_worker: 1
        # Number of GPUs to allocate per worker. This can be fractional. This is
        # usually needed only if your env itself requires a GPU (i.e., it is a
        # GPU-intensive video game), or model inference is unusually expensive.
        num_gpus_per_worker: 0.1
        # Number of CPUs to allocate for the trainer. Note: this only takes effect
        # when running in Tune. Otherwise, the trainer runs in the main program.
        num_cpus_for_driver: 1

        # === Settings for the Trainer process ===
        # Number of GPUs to allocate to the trainer process. Note that not all
        # algorithms can take advantage of trainer GPUs. This can be fractional
        # (e.g., 0.3 GPUs).
        # Note : If GPUs are not available, this will be overriden by the AIcrowd evaluators to 0.
        num_gpus: 0.3

        # === Exploration Settings ===
        # Default exploration behavior, iff `explore`=None is passed into
        # compute_action(s).
        # Set to False for no exploration behavior (e.g., for evaluation).
        explore: True,
        # Provide a dict specifying the Exploration object's config.
        exploration_config:
            # The Exploration class to use. In the simplest case, this is the name
            # (str) of any class present in the `rllib.utils.exploration` package.
            # You can also provide the python class directly or the full location
            # of your class (e.g. "ray.rllib.utils.exploration.epsilon_greedy.
            # EpsilonGreedy)
            type: "StochasticSampling"
            # Can add constructor kwargs here (if any)

        # === Advanced Rollout Settings ===
        # Element-wise observation filter, either "NoFilter" or "MeanStdFilter".
        observation_filter: "NoFilter"
        # Whether to synchronize the statistics of remote filters.
        synchronize_filters: True
        # Whether to LZ4 compress individual observations
        compress_observations: False
        # Minimum env steps to optimize for per train call. This value does
        # not affect learning, only the length of train iterations.
        timesteps_per_iteration: 0
        # This argument, in conjunction with worker_index, sets the random seed of
        # each worker, so that identically configured trials will have identical
        # results. This makes experiments reproducible.
        seed: null
        
